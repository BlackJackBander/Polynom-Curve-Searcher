# Polynom-Curve-Searcher

### Общее описание

Код генерирует набор случайных точек на плоскости, затем подбирает полиномиальную модель, которая **идеально** проходит через все эти точки (поскольку степень полинома `n-1` гарантирует точное соответствие `n` точкам), и визуализирует результат.

Отличное предложение! Добавим математическое обоснование в ключевые моменты кода.

### Общее математическое описание

Код решает задачу **интерполяции**: для набора из `n` точек $(x_i, y_i)$ находит полином $P(x)$ степени $n-1$, такой что:
$$P(x_i) = y_i \quad \forall i = 1, 2, ..., n$$

---

### Ключевые моменты с математическими формулами:

#### 1. **Полиномиальная модель (Функция `fit_polynomial`)**

**Математическая формула:**
Искомый полином степени $k = n - 1$ имеет вид:
$$P(x) = \beta_0 + \beta_1x + \beta_2x^2 + \cdots + \beta_kx^k$$

**Задача метода наименьших квадратов (МНК):** Найти коэффициенты $\beta_0, \beta_1, ..., \beta_k$, которые минимизируют сумму квадратов ошибок:
$$\min_{\beta} \sum_{i=1}^{n} [y_i - P(x_i)]^2$$

**Матричная форма:** Задача сводится к решению системы линейных уравнений:
$$X^TX\beta = X^Ty$$
где $X$ — **матрица Вандермонда**:
$$
X = \begin{pmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^k \\
1 & x_2 & x_2^2 & \cdots & x_2^k \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^k \\
\end{pmatrix}
$$

**В коде:** Функция `lm(y ~ poly(x, degree, raw = TRUE))` автоматически создает матрицу X и решает эту систему.

```r
# Математика за кодом:
# poly(x, degree, raw=TRUE) создает матрицу признаков [1, x, x², ..., xᵏ]
# lm() решает систему XᵀXβ = Xᵀy для нахождения коэффициентов β
model <- lm(poly_formula, data = points)
```

#### 2. **Вычисление ошибки (Функция `calculate_rmse`)**

**Формула RMSE (Root Mean Square Error):**
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
где $\hat{y}_i = P(x_i)$ — предсказанное значение моделью.

**Для интерполяции:** Поскольку $P(x_i) = y_i$ для всех точек, теоретически:
$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - y_i)^2} = 0$$

**В коде:**
```r
# Математика: RMSE = sqrt(mean((y - ŷ)²))
sqrt(mean((predicted - points$y)^2))
```

#### 3. **Уравнение полинома (Функция `get_polynomial_equation`)**

**Общий вид полинома:**
$$P(x) = \beta_0 + \beta_1x + \beta_2x^2 + \cdots + \beta_kx^k$$

**В коде:** Функция динамически формирует это уравнение на основе найденных коэффициентов:
```r
# Для каждого коэффициента βᵢ формируется член уравнения:
equation <- paste0(equation, sign, value, "x^", i)
```

#### 4. **Теоретическое обоснование интерполяции**

**Теорема:** Для любого набора из $n$ точек с различными $x$-координатами существует единственный полином степени не выше $n-1$, проходящий через все эти точки.

**Доказательство:** Существование следует из возможности построения интерполяционного полинома Лагранжа или решения системы с матрицей Вандермонда (которая будет невырожденной при различных $x_i$).

#### 5. **Проблема переобучения (Ключевой вывод)**

**Математическая иллюстрация:** Полином высокой степени $P(x)$ идеально описывает тренировочные данные:
$$P(x_i) = y_i \quad \text{для } i=1,...,n$$

**Но** для новой точки $x_{new}$ ошибка обобщения может быть большой из-за **явления Рунге**: колебания полинома высокой степени между узлами интерполяции могут быть очень большими.

**Феномен Рунге:** Для равноотстоящих узлов интерполяции (как в нашем случае с `runif`) ошибка интерполяции $\max|f(x) - P(x)|$ может неограниченно возрастать с увеличением степени полинома $n$, даже если функция $f(x)$ является гладкой.

---

### Итоговые математические выводы:

1.  **Интерполяционный полином:** $P(x) = \beta_0 + \beta_1x + \cdots + \beta_{n-1}x^{n-1}$
2.  **Матрица системы:** Матрица Вандермонда $X$ размера $n \times n$
3.  **Решение МНК:** $\beta = (X^TX)^{-1}X^Ty$ (дает точное решение при степени $n-1$)
4.  **Идеальное соответствие:** $P(x_i) = y_i \Rightarrow RMSE = 0$
5.  **Цена интерполяции:** Высокий риск переобучения и большие ошибки на новых данных из-за колебаний полинома высокой степени.

Этот код является наглядной демонстрацией фундаментального компромисса в машинном обучении: между точностью на тренировочных данных и способностью к обобщению.

---

### Что делает код (пошагово):

1.  **Инициализация (`set.seed`)**: Устанавливает начальное значение для генератора случайных чисел. Это обеспечивает **воспроизводимость** результатов: при каждом запуске кода будут генерироваться одни и те же "случайные" точки.

2.  **Генерация точек (`generate_random_points`)**: Создает data frame с `n` случайными точками. Координаты `x` и `y` генерируются из равномерного распределения в заданных диапазонах (по умолчанию от -10 до 10).

3.  **Подбор модели (`fit_polynomial`)**: Это **ключевая функция**.
    *   Она вычисляет необходимую степень полинома: `degree = n - 1`. Для 5 точек это полином 4-й степени.
    *   С помощью функции `lm()` (Linear Models) строится модель **полиномиальной регрессии**. Аргумент `raw = TRUE` в функции `poly()` означает, что используются обычные полиномиальные члены (`x`, `x^2`, `x^3`, ...), а не ортогональные, что важно для получения читаемого уравнения.

4.  **Расчет ошибки (`calculate_rmse`)**: Вычисляет среднеквадратическую ошибку (Root Mean Square Error). В данном конкретном случае, благодаря интерполяции, **RMSE будет всегда равна 0 (или очень близка к нулю из-за численных округлений)**, так как кривая проходит точно через все точки.

5.  **Формирование уравнения (`get_polynomial_equation`)**: Извлекает коэффициенты подобранной модели (`coef(model)`) и форматирует их в виде читаемого строкового уравнения полинома.

6.  **Визуализация (`visualize_fit`)**: С помощью библиотеки `ggplot2` создает график, на котором:
    *   Красными точками отображаются исходные данные.
    *   Синей линией рисуется предсказание модели — гладкая кривая полинома.
    *   В заголовке выводится уравнение полинома и значение RMSE.

7.  **Запуск и вывод**: В основном блоке задается количество точек, генерируются данные, строится модель и выводятся подробные результаты в консоль.

---

### Ключевые моменты (Вывод):

1.  **Интерполяция vs. Аппроксимация**: Код является наглядной демонстрацией **интерполяции** — нахождения функции, *точно* проходящей через заданный набор точек. Это отличается от *аппроксимации*, где мы ищем функцию, которая лишь *приближенно* описывает данные, минимизируя общую ошибку (часто с меньшей степенью полинома, чем `n-1`).

2.  **Проблема переобучения (Overfitting)**: Этот код — классический пример переобучения на тренировочных данных. Модель идеально описывает имеющиеся 6 точек, но будет крайне плохо предсказывать значения для любых новых, невиданных точек между ними. Кривая будет сильно изгибаться, чтобы пройти через все точки. Это главный недостаток использования полиномов высоких степеней для небольшого количества данных.

3.  **Связь количества точек и степени полинома**: Степень полинома жестко задана формулой `n - 1`. Это центральная идея, вокруг которой построен весь эксперимент.

4.  **Использование `lm()` и `poly()`**: Код показывает мощь встроенных функций R для построения линейных моделей. Комбинация `lm()` и `poly()` — это стандартный и эффективный способ проведения полиномиальной регрессии.

5.  **Воспроизводимость результатов**: Использование `set.seed(123)` — это важнейшая практика в Data Science для отладки и сравнения моделей.

**Итог:** Код создает идеальную интерполяционную кривую для случайного набора точек, наглядно демонстрируя как силу полиномиальных моделей, так и опасность переобучения.

Пример для 6 точек:

<img width="733" height="508" alt="изображение" src="https://github.com/user-attachments/assets/5c312396-8373-4922-aa00-5f966339f839" />

Чтобы развить эту тему, я применил этот подход к цене акции MSFT:
<img width="935" height="442" alt="изображение" src="https://github.com/user-attachments/assets/b52226dd-8916-4c2a-92cd-8d2330e04f1a" />

Уравнение степеней полинома содержит NA, но в нашем случае это нормально, т.к. количество точек достаточно велико.
График строится по 5 точкам максимума и минимума за 2025 год. Получился такой аналог сглаженной скользящей средней.


И еще пример для акции GOOGLE с 2024 года:
<img width="935" height="442" alt="изображение" src="https://github.com/user-attachments/assets/d263319e-b028-45aa-a6f1-8d37b6c2cc72" />

Количество экстремумов: 34
Степень полинома: 5
RMSE: 14.60256 
```r
> print(head(points_df, 5))
       x      y    type
1  19751 154.84 Maximum
18 19753 141.80 Minimum
2  19762 150.22 Maximum
19 19788 132.56 Minimum
3  19824 160.79 Maximum
```

В отдельном коде - реализация сплайнов, за что Грейс Ваба получила нобелевскую премию по статистике (см. мою статью на https://t.me/DrunkenPin)

<img width="935" height="504" alt="изображение" src="https://github.com/user-attachments/assets/fbf21f2c-e5d1-44ee-9de2-97efd9c7e3fa" />

```r
=== ИНФОРМАЦИЯ О МОДЕЛЯХ СПЛАЙНОВ ===

Модель 1: Натуральный сплайн с 4 степенями свободы
Количество точек: 34 
Степеней свободы: 4 
R-squared: 0.3167084 
RMSE: 14.41482 

Модель 2: Автоматический подбор сплайна
Количество точек: 34 
Фактическое количество степеней свободы: 6 
R-squared: 0.5255929 
RMSE: 12.01107 

Сравнительная таблица:
         Модель Точки Степени_свободы        R2     RMSE
1 Сплайн (df=4)    34               4 0.3167084 14.41482
2 Сплайн (авто)    34               6 0.5255929 12.01107

Первые 5 экстремумов:
       x      y    type
1  19751 154.84 Maximum
18 19753 141.80 Minimum
2  19762 150.22 Maximum
19 19788 132.56 Minimum
3  19824 160.79 Maximum

=== ДИАГНОСТИКА ЛУЧШЕЙ МОДЕЛИ ===
Лучшая модель: Автоматический сплайн
Коэффициенты лучшей модели:
          (Intercept) ns(x, df = df_value)1 ns(x, df = df_value)2 
           141.150763             12.544157             45.835906 
ns(x, df = df_value)3 ns(x, df = df_value)4 ns(x, df = df_value)5 
            45.193931             -9.757176             75.376536 
ns(x, df = df_value)6 
            42.579664
```
